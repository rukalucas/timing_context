# Orthogonal Subspaces Continual Learning Configuration
# Trains instructed -> inferred sequentially with gradient projection, based on Duncker et al. (2020)

trainer_type: orthogonal_sequential
random_seed: 42

wandb:
  project: "timing_context"
  offline: false  # Set to true for offline mode

start_from: logs/sequential_multitask/checkpoint_after_instructed.pt

tasks:
  - task_type: instructed # start from pretrained
    num_steps: 1000
    task:
      trials_per_sequence: 40

  - task_type: inferred
    num_steps: 2000
    task:
      trials_per_sequence: 40

model:
  hidden_size: 256
  tau: 100.0
  activation: elu

training:
  learning_rate: 0.0003
  batch_size: 128
  log_interval: 20
  num_eval_samples: 128
  checkpoint_interval: 500
  log_dir: logs/orthogonal
  optimizer_type: adam
  # Trainer-specific parameters
  reset_optimizer_between_tasks: true
  alpha_projection: 0.001
  apply_proj_to: both
  projection_collection_batch_size: 128
